{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b1224d-4b79-4680-a6f0-cd3018e67ce2",
   "metadata": {},
   "source": [
    "# Import Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3495672c-5ade-431e-8edf-d998bc5439a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk import numpy as np\n",
    " import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabac098-8e41-4bf2-aa50-d5f343c9f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.4\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m347.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n",
      "2.2.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install --no-cache-dir --force-reinstall numpy==1.24.4 --upgrade --ignore-installed\n",
    "print(np.__version__)  # should be 1.24.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90859277-1231-45d3-b5bd-9cccf36a0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "#cleaning\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce66e5-40de-47d8-9d99-b3c1a8db1a7d",
   "metadata": {},
   "source": [
    "### Unzipping Main Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcaf8fab-243a-446d-b8c3-8a5e6c480c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset']\n"
     ]
    }
   ],
   "source": [
    "bucket = 'msba-trends-group-trendnav'\n",
    "zip_key = 'Dataset-20250422T012041Z-001.zip'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.get_object(Bucket=bucket, Key=zip_key)\n",
    "\n",
    "# Extract directly from S3 stream\n",
    "with zipfile.ZipFile(io.BytesIO(response['Body'].read())) as zip_ref:\n",
    "    zip_ref.extractall('unzipped_s3_data')\n",
    "\n",
    "# List files\n",
    "print(os.listdir('unzipped_s3_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a628c321-b584-41ab-9782-f4f4bf9934f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzipped_s3_data/Dataset/Other Datasets.zip\n",
      "unzipped_s3_data/Dataset/reddit_trending_products.json\n",
      "unzipped_s3_data/Dataset/reddit_trending_products_v2.json\n",
      "unzipped_s3_data/Dataset/retail_sales_dataset.csv\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns.zip\n"
     ]
    }
   ],
   "source": [
    "folder_list=[]\n",
    "for root, dirs, files in os.walk('unzipped_s3_data'):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))\n",
    "        folder_list.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0502f5-f374-4a4d-8750-523f331b0612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qa_Appliances.json', 'qa_Musical_Instruments.json', 'qa_Patio_Lawn_and_Garden.json', 'qa_Pet_Supplies.json', 'qa_Video_Games.json', 'qa_Home_and_Kitchen.json', 'qa_Grocery_and_Gourmet_Food.json', 'qa_Tools_and_Home_Improvement.json', 'qa_Arts_Crafts_and_Sewing.json', 'qa_Electronics.json', 'qa_Office_Products.json', 'qa_Software.json', 'qa_Beauty.json', 'qa_Baby.json', 'qa_Toys_and_Games.json', 'qa_Industrial_and_Scientific.json', 'qa_Clothing_Shoes_and_Jewelry.json', 'qa_Cell_Phones_and_Accessories.json', 'qa_Automotive.json', 'qa_Sports_and_Outdoors.json', 'qa_Health_and_Personal_Care.json', '__MACOSX']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "zip_to_extract = 'unzipped_s3_data/Dataset/Amazon-QuesAns.zip'\n",
    "with zipfile.ZipFile(zip_to_extract, 'r') as zip_ref:\n",
    "    zip_ref.extractall('unzipped_s3_data/Dataset/Amazon-QuesAns')\n",
    "\n",
    "# List contents\n",
    "import os\n",
    "print(os.listdir('unzipped_s3_data/Dataset/Amazon-QuesAns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6f2b3-677e-4735-a350-a461f831b709",
   "metadata": {},
   "source": [
    "### Load Data into Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d433a9-1bd5-4d97-9060-be6afa3a450d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>flair</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuyItForLife</td>\n",
       "      <td>Me with the Golden Taco the day I bought it vs...</td>\n",
       "      <td>14466</td>\n",
       "      <td>448</td>\n",
       "      <td>https://i.redd.it/lsbf6wap5fte1.jpeg</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>1jtlx8r</td>\n",
       "      <td>2025-04-07 14:00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuyItForLife</td>\n",
       "      <td>This 1980's marker, still better than any mode...</td>\n",
       "      <td>9823</td>\n",
       "      <td>235</td>\n",
       "      <td>https://i.redd.it/n65twfbq1hue1.jpeg</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>1jxrtwd</td>\n",
       "      <td>2025-04-12 21:25:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title  score  \\\n",
       "0  BuyItForLife  Me with the Golden Taco the day I bought it vs...  14466   \n",
       "1  BuyItForLife  This 1980's marker, still better than any mode...   9823   \n",
       "\n",
       "   num_comments                                   url    flair       id  \\\n",
       "0           448  https://i.redd.it/lsbf6wap5fte1.jpeg  Vintage  1jtlx8r   \n",
       "1           235  https://i.redd.it/n65twfbq1hue1.jpeg  Vintage  1jxrtwd   \n",
       "\n",
       "           created_at  \n",
       "0 2025-04-07 14:00:09  \n",
       "1 2025-04-12 21:25:32  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load raw JSON\n",
    "with open('unzipped_s3_data/Dataset/reddit_trending_products.json') as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "# Step 2: Convert each column (dict) to a list, then zip into rows\n",
    "columns = list(raw.keys())\n",
    "records = list(zip(*[list(raw[col].values()) for col in columns]))\n",
    "\n",
    "# Step 3: Reconstruct DataFrame\n",
    "reddit = pd.DataFrame(records, columns=columns)\n",
    "reddit['created_at'] = pd.to_datetime(reddit['created_utc'], unit='ms')\n",
    "reddit = reddit.drop(columns='created_utc')  # optional: drop the old column\n",
    "\n",
    "reddit.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1955de-fa74-4eac-bae4-83077d0d5141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qa_Appliances.json', 'qa_Musical_Instruments.json', 'qa_Patio_Lawn_and_Garden.json', 'qa_Pet_Supplies.json', 'qa_Video_Games.json', 'qa_Home_and_Kitchen.json', 'qa_Grocery_and_Gourmet_Food.json', 'qa_Tools_and_Home_Improvement.json', 'qa_Arts_Crafts_and_Sewing.json', 'qa_Electronics.json', 'qa_Office_Products.json', 'qa_Software.json', 'qa_Beauty.json', 'qa_Baby.json', 'qa_Toys_and_Games.json', 'qa_Industrial_and_Scientific.json', 'qa_Clothing_Shoes_and_Jewelry.json', 'qa_Cell_Phones_and_Accessories.json', 'qa_Automotive.json', 'qa_Sports_and_Outdoors.json', 'qa_Health_and_Personal_Care.json', '__MACOSX']\n",
      "Empty DataFrame\n",
      "Columns: [asin, question, answer, questionType, answerType, answerTime, unixTime, category]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('unzipped_s3_data/Dataset/Amazon-QuesAns'))\n",
    "col_list = ['asin', 'question', 'answer', 'questionType', 'answerType', 'answerTime', 'unixTime','category']\n",
    "amazon_qa = pd.DataFrame(columns=col_list)\n",
    "print(amazon_qa)\n",
    "\n",
    "def amazon_qa_fetch(filename_qa,col_list,cat):\n",
    "    global amazon_qa\n",
    "    records = []\n",
    "    fail_count = 0\n",
    "    with open(filename_qa, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                # Use ast.literal_eval for Python-like dict parsing\n",
    "                parsed = ast.literal_eval(line.strip())\n",
    "                records.append(parsed)\n",
    "            except Exception as e:\n",
    "                print(f\" Failed on line {i}: {e}\")\n",
    "                fail_count += 1\n",
    "    print(filename_qa)\n",
    "    print(f\"Parsed {len(records)} records\")\n",
    "    print(f\"Skipped {fail_count} lines\")\n",
    "\n",
    "    amazon_qa_subgroup = pd.DataFrame(records)\n",
    "    amazon_qa_subgroup['category'] = cat.replace('.json', '')\n",
    "    amazon_qa_subgroup = amazon_qa_subgroup[col_list] \n",
    "    if set(amazon_qa.columns) == set(amazon_qa_subgroup.columns):\n",
    "        amazon_qa = pd.concat([amazon_qa, amazon_qa_subgroup]).drop_duplicates().reset_index(drop=True)\n",
    "    return amazon_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f698bf5-8496-4349-98fa-67db1f93f05a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_Appliances.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Appliances.json\n",
      "Parsed 9011 records\n",
      "Skipped 0 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34938/1954438945.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  amazon_qa = pd.concat([amazon_qa, amazon_qa_subgroup]).drop_duplicates().reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_Musical_Instruments.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Musical_Instruments.json\n",
      "Parsed 23322 records\n",
      "Skipped 0 lines\n",
      "qa_Patio_Lawn_and_Garden.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Patio_Lawn_and_Garden.json\n",
      "Parsed 59595 records\n",
      "Skipped 0 lines\n",
      "qa_Pet_Supplies.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Pet_Supplies.json\n",
      "Parsed 36607 records\n",
      "Skipped 0 lines\n",
      "qa_Video_Games.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Video_Games.json\n",
      "Parsed 13307 records\n",
      "Skipped 0 lines\n",
      "qa_Home_and_Kitchen.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Home_and_Kitchen.json\n",
      "Parsed 184439 records\n",
      "Skipped 0 lines\n",
      "qa_Grocery_and_Gourmet_Food.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Grocery_and_Gourmet_Food.json\n",
      "Parsed 19538 records\n",
      "Skipped 0 lines\n",
      "qa_Tools_and_Home_Improvement.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Tools_and_Home_Improvement.json\n",
      "Parsed 101088 records\n",
      "Skipped 0 lines\n",
      "qa_Arts_Crafts_and_Sewing.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Arts_Crafts_and_Sewing.json\n",
      "Parsed 21262 records\n",
      "Skipped 0 lines\n",
      "qa_Electronics.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Electronics.json\n",
      "Parsed 314263 records\n",
      "Skipped 0 lines\n",
      "qa_Office_Products.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Office_Products.json\n",
      "Parsed 43608 records\n",
      "Skipped 0 lines\n",
      "qa_Software.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Software.json\n",
      "Parsed 10636 records\n",
      "Skipped 0 lines\n",
      "qa_Beauty.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Beauty.json\n",
      "Parsed 42422 records\n",
      "Skipped 0 lines\n",
      "qa_Baby.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Baby.json\n",
      "Parsed 28933 records\n",
      "Skipped 0 lines\n",
      "qa_Toys_and_Games.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Toys_and_Games.json\n",
      "Parsed 51486 records\n",
      "Skipped 0 lines\n",
      "qa_Industrial_and_Scientific.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Industrial_and_Scientific.json\n",
      "Parsed 12136 records\n",
      "Skipped 0 lines\n",
      "qa_Clothing_Shoes_and_Jewelry.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Clothing_Shoes_and_Jewelry.json\n",
      "Parsed 22068 records\n",
      "Skipped 0 lines\n",
      "qa_Cell_Phones_and_Accessories.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Cell_Phones_and_Accessories.json\n",
      "Parsed 85865 records\n",
      "Skipped 0 lines\n",
      "qa_Automotive.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Automotive.json\n",
      "Parsed 89923 records\n",
      "Skipped 0 lines\n",
      "qa_Sports_and_Outdoors.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Sports_and_Outdoors.json\n",
      "Parsed 146891 records\n",
      "Skipped 0 lines\n",
      "qa_Health_and_Personal_Care.json\n",
      "unzipped_s3_data/Dataset/Amazon-QuesAns/qa_Health_and_Personal_Care.json\n",
      "Parsed 80496 records\n",
      "Skipped 0 lines\n"
     ]
    }
   ],
   "source": [
    "qa_file_list=os.listdir('unzipped_s3_data/Dataset/Amazon-QuesAns')\n",
    "for cat in qa_file_list:\n",
    "    if '.json' in cat:\n",
    "        file='unzipped_s3_data/Dataset/Amazon-QuesAns/'+str(cat)\n",
    "        print(cat)\n",
    "        amazon_qa_fetch(file,col_list,cat)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e334f4ce-dd9f-4c8d-a85e-bd9192b8d568",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "r_file_list=os.listdir('unzipped_s3_data/Dataset/Amazon-Final')\n",
    "\n",
    "for i in r_file_list:\n",
    "    if '.jsonl' in i :\n",
    "        file='unzipped_s3_data/Dataset/Amazon-Final/'+str(i)\n",
    "        amazon_r_fetch(file,col_list,i)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a454143-44cd-4d6a-946b-241c8284cc65",
   "metadata": {},
   "source": [
    "col_list = ['rating','title','category','text','images','asin','parent_asin','user_id','timestamp','helpful_vote','verified_purchase']\n",
    "amazon_r = pd.DataFrame(columns=col_list)\n",
    "print(amazon_r)\n",
    "def amazon_r_fetch(filename_r,col_list,i):\n",
    "    global amazon_r\n",
    "    fail_count=0\n",
    "    amazon_r_subgroup=pd.read_json(filename_r,lines=True)\n",
    "    print(filename_r)\n",
    "    print(f\"Skipped {fail_count} lines\")\n",
    "    amazon_r_subgroup['category'] = i.replace('.jsonl', '')\n",
    "    amazon_r_subgroup = amazon_r_subgroup[col_list] \n",
    "    if set(amazon_r.columns) == set(amazon_r_subgroup.columns):\n",
    "        amazon_r = pd.concat([amazon_r, amazon_r_subgroup]).drop_duplicates().reset_index(drop=True)\n",
    "    print(amazon_r.head(1))\n",
    "    return amazon_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a06adcb-65b7-4cf2-abef-b26b21b66de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>questionType</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>I have a 9 year old Badger 1 that needs replac...</td>\n",
       "      <td>I replaced my old one with this without a hitch.</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>?</td>\n",
       "      <td>Jun 27, 2014</td>\n",
       "      <td>1.403852e+09</td>\n",
       "      <td>qa_Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>model number</td>\n",
       "      <td>This may help InSinkErator Model BADGER-1: Bad...</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 28, 2014</td>\n",
       "      <td>1.398668e+09</td>\n",
       "      <td>qa_Appliances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>can I replace Badger 1 1/3 with a Badger 5 1/2...</td>\n",
       "      <td>Plumbing connections will vary with different ...</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>?</td>\n",
       "      <td>Aug 25, 2014</td>\n",
       "      <td>1.408950e+09</td>\n",
       "      <td>qa_Appliances</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  B00004U9JP  I have a 9 year old Badger 1 that needs replac...   \n",
       "1  B00004U9JP                                       model number   \n",
       "2  B00004U9JP  can I replace Badger 1 1/3 with a Badger 5 1/2...   \n",
       "\n",
       "                                              answer questionType answerType  \\\n",
       "0   I replaced my old one with this without a hitch.       yes/no          ?   \n",
       "1  This may help InSinkErator Model BADGER-1: Bad...   open-ended        NaN   \n",
       "2  Plumbing connections will vary with different ...       yes/no          ?   \n",
       "\n",
       "     answerTime      unixTime       category  \n",
       "0  Jun 27, 2014  1.403852e+09  qa_Appliances  \n",
       "1  Apr 28, 2014  1.398668e+09  qa_Appliances  \n",
       "2  Aug 25, 2014  1.408950e+09  qa_Appliances  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_qa.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0b000-6def-4b6e-821c-d95a50f8800d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66746d63-605d-4bbb-a4a1-d39ae1cb8d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "amazon_qa['created_at'] = pd.to_datetime(amazon_qa['unixTime'], unit='s')\n",
    "amazon_qa\n",
    "amazon_review=amazon_qa.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d82de2d8-11e3-4fca-8130-dfdd0e25fec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>questionType</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>category</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>I have a 9 year old Badger 1 that needs replac...</td>\n",
       "      <td>I replaced my old one with this without a hitch.</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>?</td>\n",
       "      <td>Jun 27, 2014</td>\n",
       "      <td>1.403852e+09</td>\n",
       "      <td>qa_Appliances</td>\n",
       "      <td>2014-06-27 07:00:00</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>model number</td>\n",
       "      <td>This may help InSinkErator Model BADGER-1: Bad...</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 28, 2014</td>\n",
       "      <td>1.398668e+09</td>\n",
       "      <td>qa_Appliances</td>\n",
       "      <td>2014-04-28 07:00:00</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  B00004U9JP  I have a 9 year old Badger 1 that needs replac...   \n",
       "1  B00004U9JP                                       model number   \n",
       "\n",
       "                                              answer questionType answerType  \\\n",
       "0   I replaced my old one with this without a hitch.       yes/no          ?   \n",
       "1  This may help InSinkErator Model BADGER-1: Bad...   open-ended        NaN   \n",
       "\n",
       "     answerTime      unixTime       category          created_at    year  \n",
       "0  Jun 27, 2014  1.403852e+09  qa_Appliances 2014-06-27 07:00:00  2014.0  \n",
       "1  Apr 28, 2014  1.398668e+09  qa_Appliances 2014-04-28 07:00:00  2014.0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_review['year'] = amazon_review['created_at'].dt.year\n",
    "amazon_review.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "affe0a38-3ef0-44d8-a861-fa4d817180bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2006.0      1257\n",
       "2007.0      2809\n",
       "2008.0      3697\n",
       "2009.0      4696\n",
       "2010.0      4873\n",
       "2011.0      5195\n",
       "2012.0     16493\n",
       "2013.0    337987\n",
       "2014.0    759305\n",
       "2015.0    198038\n",
       "Name: asin, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_review.groupby(['year'])['asin'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4f0d8846-4c33-4ff5-98cb-822a42cf04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_filt=amazon_review[amazon_review['year']>=2014.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03b532ef-fa37-4a2e-8c0c-b1e1ecf7cda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt.year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b385ae26-554b-4812-824a-d9daf48532d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34938/2728456877.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_filt['category']=amazon_filt['category'].str.replace('qa_', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Appliances', 'Musical_Instruments', 'Patio_Lawn_and_Garden',\n",
       "       'Pet_Supplies', 'Video_Games', 'Home_and_Kitchen',\n",
       "       'Grocery_and_Gourmet_Food', 'Tools_and_Home_Improvement',\n",
       "       'Arts_Crafts_and_Sewing', 'Electronics', 'Office_Products',\n",
       "       'Software', 'Beauty', 'Baby', 'Toys_and_Games',\n",
       "       'Industrial_and_Scientific', 'Clothing_Shoes_and_Jewelry',\n",
       "       'Cell_Phones_and_Accessories', 'Automotive', 'Sports_and_Outdoors',\n",
       "       'Health_and_Personal_Care'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt['category']=amazon_filt['category'].str.replace('qa_', '')\n",
    "amazon_filt.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42bd183a-fff0-4841-a119-17856a43543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_filt=amazon_filt[amazon_filt['category'].isin(['Home_and_Kitchen','Software','Cell_Phones_and_Accessories', 'Electronics','Health_and_Personal_Care',\n",
    "      'Toys_and_Games', 'Clothing_Shoes_and_Jewelry', 'Beauty', 'Appliances', 'Video_Games'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d384333-1fa8-412f-9600-66712a371ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73798, 10)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f422df0-f30c-46aa-843f-336bb41be6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_filt.to_csv('amazon_filt2.csv')\n",
    "#amazon_filt=pd.read_csv('amazon_filt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7fb26635-36f9-408c-80c4-83d66b7ef66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#amazon_filt.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f3cf4183-0189-4d6f-890a-7db1d9e58dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Appliances                       6592\n",
       "Arts_Crafts_and_Sewing          15291\n",
       "Automotive                      65396\n",
       "Baby                            20105\n",
       "Beauty                          30672\n",
       "Cell_Phones_and_Accessories     54227\n",
       "Clothing_Shoes_and_Jewelry      16743\n",
       "Electronics                    201566\n",
       "Grocery_and_Gourmet_Food        13833\n",
       "Health_and_Personal_Care        57973\n",
       "Home_and_Kitchen               123229\n",
       "Industrial_and_Scientific        9063\n",
       "Musical_Instruments             16894\n",
       "Office_Products                 30387\n",
       "Patio_Lawn_and_Garden           42585\n",
       "Pet_Supplies                    26116\n",
       "Software                         5155\n",
       "Sports_and_Outdoors            105995\n",
       "Tools_and_Home_Improvement      72805\n",
       "Toys_and_Games                  33976\n",
       "Video_Games                      8740\n",
       "Name: question, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt.groupby('category')['question'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "036cd3a2-6156-4bd0-bdaa-3537046ec9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt.year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9655330a-f971-4521-833c-906116f1e594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Vintage', 'Currently sold', 'Review', 'Discussion', '[Request]',\n",
       "       'Repair', 'Meta', '[Request] Answered!', 'BIFL Skills', 'Warranty',\n",
       "       'üëöClothing & Shoes ', 'üçé Food ', 'üöø Personal Care ',\n",
       "       'üè† Home & Apartment ', 'üí¨ Meta Discussion ',\n",
       "       '‚ôªÔ∏è Recycling & Zero-Waste ', '‚õπÔ∏è Hobbies', 'üöß DIY & Repair ',\n",
       "       'üí∞ Finance & Bills', 'üöó Auto ', 'üéì\\xa0Education / Philosophy',\n",
       "       'üå± Gardening ', 'üßí Children & Childcare ', 'üì± Phone & Internet',\n",
       "       'üì¶ Secondhand', 'üèÜ Buy It For Life ', 'üßΩ Cleaning & Organization',\n",
       "       'üëÄ Glasses & Contacts', '‚úàÔ∏è Travel & Transport', 'üíª Electronics ',\n",
       "       None, 'üëüFitness', 'cars & automotives', 'clothing & jewelry',\n",
       "       'computers & electronics', 'META', 'home and garden',\n",
       "       'food & drink', 'tools', 'Misc', 'Desktops / Laptops', 'Gaming',\n",
       "       'Watches', 'Phones', 'Music', 'TV / Projectors',\n",
       "       'Computer peripherals', 'Medical', 'VR / AR', 'Wearables',\n",
       "       'Transportation', 'Drones / UAVs', 'Cameras', 'Privacy',\n",
       "       'Artificial Intelligence', 'Business', 'Social Media', 'Security',\n",
       "       'Politics', 'Society', 'Crypto', 'Biotechnology', 'Hardware',\n",
       "       'Software', 'Energy', 'Machine Learning', 'Space',\n",
       "       'Networking/Telecom', 'Net Neutrality', 'ADBLOCK WARNING',\n",
       "       'Nanotech/Materials', 'Robotics/Automation', 'CREATOR',\n",
       "       'Troubleshooting', 'Miscellaneous', 'Build Help', 'Solved!',\n",
       "       'Peripherals', 'Build Upgrade', 'Build Complete',\n",
       "       'Review Megathread', 'Build Ready', 'Meme/Macro', 'Video',\n",
       "       'Question', 'Tech Support Solved', 'Screenshot',\n",
       "       'Game Image/Video', 'Story', 'Members of the PCMR', 'News/Article',\n",
       "       'Build/Battlestation', 'Pets of the PCMR', 'NSFMR', 'Giveaway',\n",
       "       'Nostalgia', 'Tech Support', 'Box', 'News', 'Article', 'Rumour',\n",
       "       'Support', 'App', 'News/Rumour', 'Accessory', 'Weekly Megathread'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_all=reddit.copy()\n",
    "reddit_all.flair.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c585006e-589e-47a2-83a3-afed148e3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-03-21 03:34:04')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_all['created_at'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6e66165e-b18e-4b11-a2a0-9b6e8f7654a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-04-20 02:30:32')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_all['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5df9f851-6be5-4096-b03c-9254caaa3ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flair\n",
       "Discussion           445\n",
       "Build Help           305\n",
       "Support              210\n",
       "üçé Food               138\n",
       "Business             122\n",
       "                    ... \n",
       "Music                  1\n",
       "Review Megathread      1\n",
       "Drones / UAVs          1\n",
       "Machine Learning       1\n",
       "Wearables              1\n",
       "Name: count, Length: 89, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_all=reddit_all[~reddit_all['flair'].isin([\"[Request]\",'Miscellaneous','Rumour','News/Rumour','Meme/Macro','Space','Politics','Misc',\n",
    "       'Networking/Telecom', 'Net Neutrality', 'Story','Screenshot','Tech Support Solved','News','BIFL Skills', 'Warranty'])]\n",
    "reddit_all.flair.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8ccad9c-0851-4798-bff2-483ca81b8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_all['title_lowercase']=reddit_all['title'].str.lower()\n",
    "reddit_all_backup=reddit_all.copy()\n",
    "reddit_all = reddit_all.drop_duplicates(subset=['title_lowercase'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2abd6d34-a760-41bc-84d0-641558887a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>url</th>\n",
       "      <th>flair</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title_lowercase</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BuyItForLife</td>\n",
       "      <td>Me with the Golden Taco the day I bought it vs...</td>\n",
       "      <td>14466</td>\n",
       "      <td>448</td>\n",
       "      <td>https://i.redd.it/lsbf6wap5fte1.jpeg</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>1jtlx8r</td>\n",
       "      <td>2025-04-07 14:00:09</td>\n",
       "      <td>me with the golden taco the day i bought it vs...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BuyItForLife</td>\n",
       "      <td>This 1980's marker, still better than any mode...</td>\n",
       "      <td>9823</td>\n",
       "      <td>235</td>\n",
       "      <td>https://i.redd.it/n65twfbq1hue1.jpeg</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>1jxrtwd</td>\n",
       "      <td>2025-04-12 21:25:32</td>\n",
       "      <td>this 1980's marker, still better than any mode...</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title  score  \\\n",
       "0  BuyItForLife  Me with the Golden Taco the day I bought it vs...  14466   \n",
       "1  BuyItForLife  This 1980's marker, still better than any mode...   9823   \n",
       "\n",
       "   num_comments                                   url    flair       id  \\\n",
       "0           448  https://i.redd.it/lsbf6wap5fte1.jpeg  Vintage  1jtlx8r   \n",
       "1           235  https://i.redd.it/n65twfbq1hue1.jpeg  Vintage  1jxrtwd   \n",
       "\n",
       "           created_at                                    title_lowercase  year  \n",
       "0 2025-04-07 14:00:09  me with the golden taco the day i bought it vs...  2025  \n",
       "1 2025-04-12 21:25:32  this 1980's marker, still better than any mode...  2025  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_all['year'] = reddit_all['created_at'].dt.year\n",
    "\n",
    "reddit_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "def87389-eccd-4df1-acba-fb5518376d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_final=reddit_all[['subreddit','title','created_at','flair','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "639f516d-d531-4fc5-a6b1-947deef754ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>created_at</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have a 9 year old Badger 1 that needs replac...</td>\n",
       "      <td>I replaced my old one with this without a hitch.</td>\n",
       "      <td>2014-06-27 07:00:00</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model number</td>\n",
       "      <td>This may help InSinkErator Model BADGER-1: Bad...</td>\n",
       "      <td>2014-04-28 07:00:00</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can I replace Badger 1 1/3 with a Badger 5 1/2...</td>\n",
       "      <td>Plumbing connections will vary with different ...</td>\n",
       "      <td>2014-08-25 07:00:00</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does this come with power cord and dishwasher ...</td>\n",
       "      <td>It does not come with a power cord. It does co...</td>\n",
       "      <td>2014-11-03 08:00:00</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loud noise inside when turned on. sounds like ...</td>\n",
       "      <td>Check if you dropped something inside.Usually ...</td>\n",
       "      <td>2014-06-21 07:00:00</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383858</th>\n",
       "      <td>Do the lights rotate by it self? Or do I have ...</td>\n",
       "      <td>Hi Fine Tuning, Deneve here. Our aroma diffuse...</td>\n",
       "      <td>2014-07-21 07:00:00</td>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383859</th>\n",
       "      <td>I see that it comes with an adapter but in the...</td>\n",
       "      <td>Yes, you need to plug it on the electrical out...</td>\n",
       "      <td>2014-08-06 07:00:00</td>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383860</th>\n",
       "      <td>does this diffuser automatically shut off when...</td>\n",
       "      <td>Hi Pam, Deneve here. Yes, our diffuser will au...</td>\n",
       "      <td>2014-07-19 07:00:00</td>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383861</th>\n",
       "      <td>Do you have to have the LED light on? Or can y...</td>\n",
       "      <td>Hi Christina, Deneve here. Our essential oil d...</td>\n",
       "      <td>2014-07-28 07:00:00</td>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383862</th>\n",
       "      <td>is the black available?</td>\n",
       "      <td>I was going to ask the same thing. I don't wan...</td>\n",
       "      <td>2015-04-20 07:00:00</td>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957343 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "0        I have a 9 year old Badger 1 that needs replac...   \n",
       "1                                             model number   \n",
       "2        can I replace Badger 1 1/3 with a Badger 5 1/2...   \n",
       "3        Does this come with power cord and dishwasher ...   \n",
       "4        loud noise inside when turned on. sounds like ...   \n",
       "...                                                    ...   \n",
       "1383858  Do the lights rotate by it self? Or do I have ...   \n",
       "1383859  I see that it comes with an adapter but in the...   \n",
       "1383860  does this diffuser automatically shut off when...   \n",
       "1383861  Do you have to have the LED light on? Or can y...   \n",
       "1383862                            is the black available?   \n",
       "\n",
       "                                                    answer  \\\n",
       "0         I replaced my old one with this without a hitch.   \n",
       "1        This may help InSinkErator Model BADGER-1: Bad...   \n",
       "2        Plumbing connections will vary with different ...   \n",
       "3        It does not come with a power cord. It does co...   \n",
       "4        Check if you dropped something inside.Usually ...   \n",
       "...                                                    ...   \n",
       "1383858  Hi Fine Tuning, Deneve here. Our aroma diffuse...   \n",
       "1383859  Yes, you need to plug it on the electrical out...   \n",
       "1383860  Hi Pam, Deneve here. Yes, our diffuser will au...   \n",
       "1383861  Hi Christina, Deneve here. Our essential oil d...   \n",
       "1383862  I was going to ask the same thing. I don't wan...   \n",
       "\n",
       "                 created_at                  category    year  \n",
       "0       2014-06-27 07:00:00                Appliances  2014.0  \n",
       "1       2014-04-28 07:00:00                Appliances  2014.0  \n",
       "2       2014-08-25 07:00:00                Appliances  2014.0  \n",
       "3       2014-11-03 08:00:00                Appliances  2014.0  \n",
       "4       2014-06-21 07:00:00                Appliances  2014.0  \n",
       "...                     ...                       ...     ...  \n",
       "1383858 2014-07-21 07:00:00  Health_and_Personal_Care  2014.0  \n",
       "1383859 2014-08-06 07:00:00  Health_and_Personal_Care  2014.0  \n",
       "1383860 2014-07-19 07:00:00  Health_and_Personal_Care  2014.0  \n",
       "1383861 2014-07-28 07:00:00  Health_and_Personal_Care  2014.0  \n",
       "1383862 2015-04-20 07:00:00  Health_and_Personal_Care  2015.0  \n",
       "\n",
       "[957343 rows x 5 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt[['question','answer','created_at','category','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "021ff9bb-14ca-4fb7-aca7-f0f50e7d65f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>questionType</th>\n",
       "      <th>answerType</th>\n",
       "      <th>answerTime</th>\n",
       "      <th>unixTime</th>\n",
       "      <th>category</th>\n",
       "      <th>created_at</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>I have a 9 year old Badger 1 that needs replac...</td>\n",
       "      <td>I replaced my old one with this without a hitch.</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>?</td>\n",
       "      <td>Jun 27, 2014</td>\n",
       "      <td>1.403852e+09</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014-06-27 07:00:00</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004U9JP</td>\n",
       "      <td>model number</td>\n",
       "      <td>This may help InSinkErator Model BADGER-1: Bad...</td>\n",
       "      <td>open-ended</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 28, 2014</td>\n",
       "      <td>1.398668e+09</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2014-04-28 07:00:00</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin                                           question  \\\n",
       "0  B00004U9JP  I have a 9 year old Badger 1 that needs replac...   \n",
       "1  B00004U9JP                                       model number   \n",
       "\n",
       "                                              answer questionType answerType  \\\n",
       "0   I replaced my old one with this without a hitch.       yes/no          ?   \n",
       "1  This may help InSinkErator Model BADGER-1: Bad...   open-ended        NaN   \n",
       "\n",
       "     answerTime      unixTime    category          created_at    year  \n",
       "0  Jun 27, 2014  1.403852e+09  Appliances 2014-06-27 07:00:00  2014.0  \n",
       "1  Apr 28, 2014  1.398668e+09  Appliances 2014-04-28 07:00:00  2014.0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_filt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "888c247d-78c9-40f5-a236-137937843904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34938/676027832.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reddit_final['source'] = 'reddit'\n",
      "/tmp/ipykernel_34938/676027832.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amazon_filt['source'] = 'amazon'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         header                                               text  \\\n",
      "0  BuyItForLife  Me with the Golden Taco the day I bought it vs...   \n",
      "1  BuyItForLife  This 1980's marker, still better than any mode...   \n",
      "2  BuyItForLife  I‚Äôve wanted one forever and finally found one ...   \n",
      "3  BuyItForLife                     Best Vacuum for Pet Hair (IMO)   \n",
      "4  BuyItForLife        NYT just put out a list of their BIFL picks   \n",
      "\n",
      "            timestamp        category    year  source  \n",
      "0 2025-04-07 14:00:09         Vintage  2025.0  reddit  \n",
      "1 2025-04-12 21:25:32         Vintage  2025.0  reddit  \n",
      "2 2025-03-26 01:54:54  Currently sold  2025.0  reddit  \n",
      "3 2025-03-25 18:30:44          Review  2025.0  reddit  \n",
      "4 2025-04-05 20:10:16          Review  2025.0  reddit  \n"
     ]
    }
   ],
   "source": [
    "#Merge\n",
    "# Step 1: Add source columns to identify data origin\n",
    "reddit_final['source'] = 'reddit'\n",
    "amazon_filt['source'] = 'amazon'\n",
    "\n",
    "# Step 2: Align column names for merging\n",
    "reddit_final_renamed = reddit_final.rename(columns={\n",
    "    'title': 'text',\n",
    "    'subreddit':'header',\n",
    "    'created_at': 'timestamp',\n",
    "    'flair': 'category',\n",
    "    'year':'year'\n",
    "})\n",
    "amazon_filt_renamed = amazon_filt.rename(columns={\n",
    "    'answer': 'text',\n",
    "    'question': 'header',\n",
    "    'created_at': 'timestamp',\n",
    "    'category': 'category',\n",
    "    'year': 'year'\n",
    "})\n",
    "\n",
    "# Reorder columns for consistency\n",
    "common_cols = ['header', 'text', 'timestamp', 'category', 'year', 'source']\n",
    "reddit_final_renamed = reddit_final_renamed[common_cols]\n",
    "amazon_filt_renamed = amazon_filt_renamed[common_cols]\n",
    "\n",
    "# Step 3: Merge the datasets\n",
    "merged_df = pd.concat([reddit_final_renamed, amazon_filt_renamed], ignore_index=True)\n",
    "\n",
    "# Step 4: Check result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686846b-af29-4a92-8a4f-6359bf9e84d2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a192f118-f55f-4948-a3ef-c043a9400f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.2.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9da789e1-4afd-4a63-b8d4-c88c4d25a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b7c733fe-05ba-4211-92c2-7dbfb4821223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Tokenizer, StopWordsRemover, SQLTransformer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.types import FloatType\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from transformers import pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "789b43ab-bc3f-4055-85e4-9351b11bb4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_for_sentiment(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove line breaks & extra spaces\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # Optionally remove URLs (only if model struggles)\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply to your DataFrame\n",
    "merged_df['cleaned_text']= merged_df['text'].apply(clean_for_sentiment)\n",
    "merged_df['cleaned_header']= merged_df['header'].apply(clean_for_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88bdebde-8161-4574-baed-5f1b0e2e7715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.30.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (4.13.1)\n",
      "Requirement already satisfied: hf-xet>=0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub[hf_xet]) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub[hf_xet]) (3.2.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adb5cd-bc90-4339-bd04-e1e15a77cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline (device=-1 = CPU; device=0 = GPU)\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,                # Change to 0 if you want to use GPU\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Correct label map from model config (these are the actual sentiment labels used by this model)\n",
    "id2label = model.config.id2label  # {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "label_map = {\n",
    "    \"LABEL_0\": -1,\n",
    "    \"LABEL_1\": 0,\n",
    "    \"LABEL_2\": 1\n",
    "}\n",
    "\n",
    "# Function to predict sentiment in batches\n",
    "def predict_bert_labels_batch(texts, batch_size=32):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            preds = classifier(batch)\n",
    "            results.extend([label_map[r[\"label\"]] for r in preds])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentiment prediction on batch {i}-{i+batch_size}: {e}\")\n",
    "            results.extend([None] * len(batch))\n",
    "    return results\n",
    "\n",
    "# Example: Assuming you already have a DataFrame named `df`\n",
    "# And you want to analyze sentiment on a column called 'cleaned_text'\n",
    "\n",
    "# Step 1: Truncate text manually (optional but safe)\n",
    "merged_df['cleaned_text'] = merged_df['cleaned_text'].fillna(\"\").apply(lambda x: x[:512])\n",
    "\n",
    "# Step 2: Run sentiment analysis\n",
    "merged_df['sentiment_score'] = predict_bert_labels_batch(merged_df['cleaned_text'].tolist())\n",
    "\n",
    "# Done: Now df has a column with -1 (neg), 0 (neutral), 1 (pos)\n",
    "print(merged_df[['cleaned_text', 'sentiment_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c74a1-9c33-4953-9929-435ad4901989",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sentiment_score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6def5f9-d0f8-43a4-922a-4c175afedf43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bf664b1-b8ba-4600-bbd5-2fa35aeb57e9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Sentiment pipeline (device=0 for GPU, use -1 for CPU)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Label mapping: HuggingFace -> Score\n",
    "hf_to_score = {\n",
    "    'LABEL_0': -1,  # Negative\n",
    "    'LABEL_1': 0,   # Neutral\n",
    "    'LABEL_2': 1    # Positive\n",
    "}\n",
    "\n",
    "# Function to truncate using tokenizer\n",
    "def truncate_text(text, max_len=512):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    encoded = tokenizer(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    return tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "# Apply truncation to your cleaned column\n",
    "merged_df['truncated_text'] = merged_df['cleaned_text'].fillna(\"\").apply(truncate_text)\n",
    "\n",
    "# Run sentiment prediction in batches\n",
    "def predict_bert_sentiment(texts, batch_size=16):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            batch_results = classifier(batch)\n",
    "            results.extend([hf_to_score.get(r['label'], None) for r in batch_results])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error at batch {i}-{i+batch_size}: {e}\")\n",
    "            results.extend([None] * len(batch))\n",
    "    return results\n",
    "\n",
    "# Predict sentiment scores\n",
    "merged_df['sentiment_score'] = predict_bert_sentiment(merged_df['truncated_text'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7bfa8-f226-4f7d-9f37-a0a004e85902",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17b543-ea54-4247-88dd-28d7690b7f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cee0a-032d-42d9-a785-cb331c64fb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d21c90-4b0c-4c4b-b97c-ba44348e0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "               tokenizer=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "               aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "# Sample data\n",
    "#merged_df = qa_df.sample(5000, random_state=42)\n",
    "merged_df[\"qa_text\"] = merged_df[\"cleaned_header\"].fillna(\"\") + \" \" + merged_df[\"cleaned_text\"].fillna(\"\")\n",
    "texts = merged_df[\"qa_text\"].astype(str).tolist()\n",
    "\n",
    "# Run NER\n",
    "batch_size = 32\n",
    "ner_entities = []\n",
    "labels_to_keep = {\"PRODUCT\", \"ORG\", \"MISC\"}\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Running NER\"):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    try:\n",
    "        results = ner(batch)\n",
    "        batch_words = [\n",
    "            [ent[\"word\"].lower() for ent in post if ent[\"entity_group\"].upper() in labels_to_keep]\n",
    "            for post in results\n",
    "        ]\n",
    "        ner_entities.extend(batch_words)\n",
    "    except Exception as e:\n",
    "        print(f\"Batch {i} error:\", e)\n",
    "        ner_entities.extend([[] for _ in batch])\n",
    "\n",
    "merged_df[\"ner_entities\"] = ner_entities\n",
    "\n",
    "# Expand entities with noun chunks\n",
    "def expand_with_noun_chunk(text, ner_words):\n",
    "    doc = nlp(text)\n",
    "    expanded = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        for word in ner_words:\n",
    "            if word in chunk.text.lower():\n",
    "                expanded.append(chunk.text.lower().strip())\n",
    "                break\n",
    "    return list(set(expanded))\n",
    "\n",
    "merged_df[\"expanded_entities\"] = merged_df.apply(\n",
    "    lambda row: expand_with_noun_chunk(row[\"qa_text\"], row[\"ner_entities\"]),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f65c4-7083-452e-bcad-958ebc0483af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Flatten and attach category\n",
    "exploded = merged_df[[\"expanded_entities\", \"category\"]].explode(\"expanded_entities\").dropna()\n",
    "\n",
    "# Smart phrase cleaning ‚Äî keep full phrases, remove only unhelpful leading words\n",
    "def smart_clean(phrase):\n",
    "    words = phrase.lower().strip().split()\n",
    "    if len(words) > 1 and words[0] in {\"the\", \"my\", \"a\", \"your\", \"this\", \"an\"}:\n",
    "        return \" \".join(words[1:])\n",
    "    return \" \".join(words)\n",
    "\n",
    "exploded[\"clean_phrase\"] = exploded[\"expanded_entities\"].apply(smart_clean)\n",
    "\n",
    "# Group & count\n",
    "grouped = (\n",
    "    exploded.groupby([\"clean_phrase\", \"category\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"Frequency\")\n",
    "            .sort_values(\"Frequency\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show result\n",
    "print(grouped.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae66081-7888-4b33-aa93-b86de0670017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explode with category and date preserved\n",
    "exploded = merged_df[[\"expanded_entities\", \"category\", \"timestamp\",\"sentiment_score\"]].explode(\"expanded_entities\").dropna()\n",
    "\n",
    "# Step 2: Clean each expanded entity\n",
    "def smart_clean(phrase):\n",
    "    words = phrase.lower().strip().split()\n",
    "    if len(words) > 1 and words[0] in {\"the\", \"my\", \"a\", \"your\", \"this\", \"an\"}:\n",
    "        return \" \".join(words[1:])\n",
    "    return \" \".join(words)\n",
    "\n",
    "exploded[\"clean_phrase\"] = exploded[\"expanded_entities\"].apply(smart_clean)\n",
    "\n",
    "# Step 3: Calculate phrase frequency (phrase + category only)\n",
    "phrase_counts = (\n",
    "    exploded.groupby([\"clean_phrase\", \"category\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"phrase_freq\")\n",
    ")\n",
    "\n",
    "# Step 4: Join phrase frequency back to the exploded data (not grouped yet)\n",
    "exploded = exploded.merge(phrase_counts, on=[\"clean_phrase\", \"category\"], how=\"left\")\n",
    "\n",
    "# Step 5: Merge back to original `merged_df`\n",
    "# First, create a unique row ID to merge on\n",
    "merged_df[\"row_id\"] = merged_df.index\n",
    "exploded[\"row_id\"] = exploded.index  # exploded keeps same index per row if exploded properly\n",
    "\n",
    "# Step 6: Group exploded back into list per row_id\n",
    "final_freq_df = exploded.groupby(\"row_id\")[[\"clean_phrase\", \"phrase_freq\"]].agg(list).reset_index()\n",
    "\n",
    "# Step 7: Merge back into merged_df\n",
    "merged_df = merged_df.merge(final_freq_df, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# Now merged_df has:\n",
    "# - original text\n",
    "# - expanded_entities\n",
    "# - clean_phrase list\n",
    "# - phrase_freq list per row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b782d7c-6995-48bd-b6d9-c3576f122acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explode expanded_entities to one row per entity, keeping metadata\n",
    "exploded = merged_df[[\"expanded_entities\", \"category\", \"timestamp\",\"sentiment_score\"]].copy()\n",
    "exploded[\"row_id\"] = exploded.index  # Keep reference to original row\n",
    "exploded = exploded.explode(\"expanded_entities\").dropna()\n",
    "\n",
    "# Step 2: Smart clean the phrase\n",
    "#def smart_clean(phrase):\n",
    "#    words = phrase.lower().strip().split()\n",
    "#    if len(words) > 1 and words[0] in {\"the\", \"my\", \"a\", \"your\", \"this\", \"an\",\"any\",\"is\",\"specifially\",\")\",\"(\"}:\n",
    "#        return \" \".join(words[1:])\n",
    "#    return \" \".join(words)\n",
    "leading_trailing_stopwords = {\n",
    "    \"the\", \"my\", \"a\", \"your\", \"this\", \"an\", \"any\", \"is\", \"specifially\",\n",
    "    \")\", \"(\", \"that\", \"to\", \"for\", \"of\", \"on\", \"in\", \"it\",\"home\",\"page\",\"these\",\"another\",\"not the\"\n",
    "}\n",
    "\n",
    "def smart_clean(phrase):\n",
    "    words = phrase.lower().strip().split()\n",
    "\n",
    "    # Remove leading junk\n",
    "    while len(words) > 1 and words[0] in leading_trailing_stopwords:\n",
    "        words = words[1:]\n",
    "\n",
    "    # Remove trailing junk\n",
    "    while len(words) > 1 and words[-1] in leading_trailing_stopwords:\n",
    "        words = words[:-1]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "exploded[\"clean_phrase\"] = exploded[\"expanded_entities\"].apply(smart_clean)\n",
    "\n",
    "# Step 3: Calculate frequency of each (clean_phrase, category)\n",
    "phrase_freq = (\n",
    "    exploded.groupby([\"clean_phrase\", \"category\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"phrase_freq\")\n",
    ")\n",
    "\n",
    "# Step 4: Merge frequency into exploded data\n",
    "exploded_final = exploded.merge(phrase_freq, on=[\"clean_phrase\", \"category\"], how=\"left\")\n",
    "\n",
    "# Step 5: Optional ‚Äî merge back to original metadata (if needed)\n",
    "# merged_df can still keep original columns; exploded_final has 1 row per phrase\n",
    "\n",
    "# Final output: exploded_final\n",
    "# Columns: [row_id, date, category, expanded_entities, clean_phrase, phrase_freq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f26c8-131a-4a97-a195-672eb5d9d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_final.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567b13f-1bf2-4998-9093-e2cab4d7f595",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accda71d-f65d-460e-b734-e394b6e6a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Extra stop nouns to remove product-irrelevant words\n",
    "common_stop_nouns = {\n",
    "    \"what\", \"anyone\", \"you\", \"people\", \"help\",\"what\",'whats',\"who\",\"when\",\"why\",\"where\", \"something\", \"thing\", \"someone\", \"everything\",\n",
    "    \"nothing\", \"everything\", \"anything\", \"everything\", \"this\", \"that\", \"one\", \"some\", \"time\",\"trump\",\"you\"\n",
    "}\n",
    "def preprocess_text(text, is_amazon=True, remove_stopwords=False):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s-]', '', text)\n",
    "\n",
    "    if not is_amazon:\n",
    "        text = re.sub(r'@[a-zA-Z0-9_]+', '', text)\n",
    "        text = re.sub(r'[:;][-o]*[\\)p\\(d]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    all_stopwords = stop_words.union(common_stop_nouns)\n",
    "\n",
    "    if remove_stopwords:\n",
    "        print(\"Before filtering:\", tokens)\n",
    "        tokens = [token for token in tokens if token not in all_stopwords]\n",
    "        print(\"After filtering:\", tokens)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    cleaned_text = ' '.join(tokens).strip()\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6fcb5-2f3f-4976-9543-e6578c526a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exploded_final['cleaned_title_tf'] = exploded_final['clean_phrase'].apply(lambda x: preprocess_text(x, is_amazon=False, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca6624-c5a3-4f64-bb0a-8357bb18f7a2",
   "metadata": {},
   "source": [
    "- Preserves product terms like \"air fryer\" as \"air_fryer\" for consistent tokenization \n",
    "- Create three clean, tokenized text columns:\n",
    "    - Column Name\tDescription\n",
    "    - TFIDF_Nouns\t: Product mentions, entities, core ideas\n",
    "    - TFIDF_Adjectives\t: Opinions, qualities\n",
    "    - TFIDF_Verbs\t: Actions, complaints, behaviors\n",
    "- Run TF-IDF on each POS slice:\n",
    "    This gives you POS-specific vocabularies, so you can:\n",
    "    - POS\tInsights\n",
    "    - **Nouns**\tWhat products/features are mentioned most uniquely?\n",
    "    - **Adjectives**\tWhat qualities or descriptors are people using?\n",
    "    - **Verbs**\tWhat do users say they're doing (e.g., ‚Äúreturned‚Äù, ‚Äúreplaced‚Äù, ‚Äúloved\")\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a80e5-3f69-41bb-94f8-4febadb40070",
   "metadata": {},
   "source": [
    "### Extract POS (nouns, adjectives, verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0f01bb0-c64b-4b11-a421-d30f8203e760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb936c1c-050a-4518-ba76-6e92efc1dbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 146, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35938ab6-34ca-443a-b034-abdae72f62d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_pos(text):\n",
    "    if not isinstance(text, str):\n",
    "        return [], [], []\n",
    "    doc = nlp(text)\n",
    "    nouns = [chunk.text.strip() for chunk in doc.noun_chunks]\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "    return nouns, adjectives, verbs\n",
    "\n",
    "# Apply POS tagging to cleaned Reddit titles\n",
    "pos_data = exploded_final['cleaned_title_tf'].apply(extract_pos)\n",
    "exploded_final['Cleaned Nouns'] = pos_data.apply(lambda x: x[0])\n",
    "exploded_final['Cleaned Adjectives'] = pos_data.apply(lambda x: x[1])\n",
    "exploded_final['Cleaned Verbs'] = pos_data.apply(lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bba898-4986-4ecf-a183-a41c2ab158d4",
   "metadata": {},
   "source": [
    "### Create TF-IDF‚Äìready text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05227cfb-795a-4347-841a-8b2c5e7560de",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_final['TFIDF_Nouns'] = exploded_final['Cleaned Nouns'].apply(\n",
    "    lambda tokens: \" \".join(token.replace(\" \", \"_\") for token in tokens)\n",
    ")\n",
    "\n",
    "exploded_final['TFIDF_Adjectives'] = exploded_final['Cleaned Adjectives'].apply(\n",
    "    lambda tokens: \" \".join(token.replace(\" \", \"_\") for token in tokens)\n",
    ")\n",
    "\n",
    "exploded_final['TFIDF_Verbs'] = exploded_final['Cleaned Verbs'].apply(\n",
    "    lambda tokens: \" \".join(token.replace(\" \", \"_\") for token in tokens)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf00e83-b96f-441e-831a-71f88c03a2c5",
   "metadata": {},
   "source": [
    "### Run TF-IDF vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1927e3-272a-4a26-b671-b0246eeef962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Nouns\n",
    "noun_vectorizer = TfidfVectorizer()\n",
    "X_nouns = noun_vectorizer.fit_transform(exploded_final['TFIDF_Nouns'])\n",
    "\n",
    "# Adjectives\n",
    "adj_vectorizer = TfidfVectorizer()\n",
    "X_adjs = adj_vectorizer.fit_transform(exploded_final['TFIDF_Adjectives'])\n",
    "\n",
    "# Verbs\n",
    "verb_vectorizer = TfidfVectorizer()\n",
    "X_verbs = verb_vectorizer.fit_transform(exploded_final['TFIDF_Verbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557b83d-8a48-4a4e-9a3c-cf8fbd431527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top_terms(vectorizer, matrix, top_n=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    sums = matrix.sum(axis=0).A1\n",
    "    top_indices = np.argsort(sums)[::-1][:top_n]\n",
    "    return [(terms[i], round(sums[i], 2)) for i in top_indices]\n",
    "\n",
    "top_nouns = get_top_terms(noun_vectorizer, X_nouns)\n",
    "top_adjs  = get_top_terms(adj_vectorizer, X_adjs)\n",
    "top_verbs = get_top_terms(verb_vectorizer, X_verbs)\n",
    "\n",
    "def plot_top_terms(top_terms, title):\n",
    "    terms, scores = zip(*top_terms)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(terms[::-1], scores[::-1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"TF-IDF Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_terms(top_nouns, \"Top Product Mentions (Nouns)\")\n",
    "plot_top_terms(top_adjs, \"Top Opinion Words (Adjectives)\")\n",
    "plot_top_terms(top_verbs, \"Top Action Words (Verbs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d342e-47a2-45f9-bf52-6678051b6231",
   "metadata": {},
   "source": [
    "# Inventory Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20b339eb-5311-49d0-ac2f-df919775be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from kagglehub) (21.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->kagglehub) (3.2.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52023d57-588d-4a2b-82c9-2b52ea0a4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"malaiarasugraj/e-commerce-dataset\")\n",
    "\n",
    "print(\"Path to dataset¬†files:\",path)\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd056e20-0d1b-4e5c-a8e2-755228d6ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomm_df_2 = pd.read_csv(os.path.join(path, \"diversified_ecommerce_dataset.csv\"))\n",
    "ecomm_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb92fa9-f3a6-412a-a52e-ea58687bb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecomm_agg=ecomm_df_2.groupby(['Product Name','Category'])['Stock Level'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60905472-9b40-423e-84d6-0a2e96ee3466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecomm_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef7c21-7a1c-422d-ba20-4078ccae808d",
   "metadata": {},
   "source": [
    "# Fuzzy Matching with Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a43db-5aaf-419f-8094-df5ab2ec87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccafd1-b085-495a-b214-c7d876ce906a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Prepare inputs\n",
    "phrases = exploded_final[['clean_phrase', 'category', 'timestamp', 'phrase_freq', 'row_id','TFIDF_Nouns']].dropna()\n",
    "phrases = exploded_final.drop_duplicates(subset=['clean_phrase'])\n",
    "\n",
    "# Step 2: Embed Reddit clean phrases + Amazon Product Names\n",
    "reddit_embeddings = model.encode(phrases['clean_phrase'].tolist(), show_progress_bar=True)\n",
    "amazon_embeddings = model.encode(ecomm_agg['Product Name'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Step 3: Match Reddit ‚Üí Amazon (Top-1 match)\n",
    "similarity = cosine_similarity(reddit_embeddings, amazon_embeddings)\n",
    "\n",
    "matches = []\n",
    "top_k = 1\n",
    "\n",
    "for i, sim_row in enumerate(similarity):\n",
    "    top_idx = np.argsort(sim_row)[::-1][:top_k][0]\n",
    "    matched_product = ecomm_agg.iloc[top_idx]\n",
    "    \n",
    "    matches.append({\n",
    "        'row_id': phrases.iloc[i]['row_id'],\n",
    "        'clean_phrase': phrases.iloc[i]['clean_phrase'],\n",
    "        'reddit_category': phrases.iloc[i]['category'],\n",
    "        'timestamp': phrases.iloc[i]['timestamp'],\n",
    "        'phrase_freq': phrases.iloc[i]['phrase_freq'],\n",
    "        'matched_amazon_product': matched_product['Product Name'],\n",
    "        'amazon_category': matched_product['Category'],\n",
    "        'stock_level': matched_product['Stock Level'],\n",
    "        'similarity': sim_row[top_idx]\n",
    "    })\n",
    "\n",
    "\n",
    "# Step 4: Final matched DataFrame\n",
    "matches_df = pd.DataFrame(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3254c-774e-4ed8-8e5a-5eb2d7ed041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Prepare clean and TFIDF noun phrases\n",
    "phrases = exploded_final[['clean_phrase', 'TFIDF_Nouns', 'category', 'timestamp', 'phrase_freq', 'row_id','sentiment_score']].dropna()\n",
    "phrases = phrases.drop_duplicates(subset=['clean_phrase'])\n",
    "\n",
    "# Initial SBERT embeddings on clean_phrase\n",
    "reddit_embeddings = model.encode(phrases['clean_phrase'].tolist(), show_progress_bar=True)\n",
    "amazon_embeddings = model.encode(ecomm_agg['Product Name'].tolist(), show_progress_bar=True)\n",
    "\n",
    "similarity = cosine_similarity(reddit_embeddings, amazon_embeddings)\n",
    "\n",
    "matches = []\n",
    "top_k = 1\n",
    "\n",
    "for i, sim_row in enumerate(similarity):\n",
    "    top_idx = np.argsort(sim_row)[::-1][:top_k][0]\n",
    "    sbert_sim = sim_row[top_idx]\n",
    "    \n",
    "    # If similarity is low, re-embed using TFIDF_Nouns instead\n",
    "    if sbert_sim < 0.5:\n",
    "        tfidf_phrase = phrases.iloc[i]['TFIDF_Nouns']\n",
    "        tfidf_embedding = model.encode([tfidf_phrase])[0]\n",
    "        sim_row_alt = cosine_similarity([tfidf_embedding], amazon_embeddings)[0]\n",
    "        top_idx = np.argsort(sim_row_alt)[::-1][0]\n",
    "        sim_used = sim_row_alt[top_idx]\n",
    "    else:\n",
    "        sim_used = sbert_sim\n",
    "\n",
    "    matched_product = ecomm_agg.iloc[top_idx]\n",
    "\n",
    "    matches.append({\n",
    "        'row_id': phrases.iloc[i]['row_id'],\n",
    "        'clean_phrase': phrases.iloc[i]['clean_phrase'],\n",
    "        'category': phrases.iloc[i]['category'],\n",
    "        'timestamp': phrases.iloc[i]['timestamp'],\n",
    "        'phrase_freq': phrases.iloc[i]['phrase_freq'],\n",
    "        'sentiment_score': phrases.iloc[i]['sentiment_score'],\n",
    "        'matched_product': matched_product['Product Name'],\n",
    "        'listing_category': matched_product['Category'],\n",
    "        'stock_level': matched_product['Stock Level'],\n",
    "        'final_similarity': sim_used,\n",
    "        'used_backup': sbert_sim < 0.5  # True if TFIDF_Nouns was used\n",
    "    })\n",
    "\n",
    "# Final match dataframe\n",
    "matches_df = pd.DataFrame(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c595159-a356-4e89-b5f4-d8223e758241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matched_filt=matches_df[matches_df['final_similarity']>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5aa96-d5ab-4d24-8fcc-a52787e883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.to_csv('matched_df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a8399-ba7e-4f3b-9f8d-f69125687d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_filt.to_csv('matched_df_final_filt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec7dcf-4675-4d85-a67a-f237aa2abd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_filt['timestamp'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a06874-9a1a-4941-be2b-3f52ce821b7a",
   "metadata": {},
   "source": [
    "# Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d24340-03a0-473b-a236-e7ea55ef9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[19]:\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# -----------------------\n",
    "# PAGE CONFIG\n",
    "# -----------------------\n",
    "st.set_page_config(page_title=\"TrendNav AI\", layout=\"wide\", page_icon=\"üìä\")\n",
    "# -----------------------\n",
    "# CUSTOM CSS STYLES (UPDATED)\n",
    "# -----------------------\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    /* Light background for the full app */\n",
    "    .stApp {\n",
    "        background-color: #FBFEFF;\n",
    "    }\n",
    "\n",
    "    /* Customize the dropdown and selected tag chips */\n",
    "    div[data-baseweb=\"select\"] > div {\n",
    "        background-color: #e6f0fa;\n",
    "        border: 1px solid #0b6da4;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "\n",
    "    /* Multiselect selected tag styles */\n",
    "    span[data-baseweb=\"tag\"] {\n",
    "        background-color: #0b6da4 !important;\n",
    "        color: white !important;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px 8px;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "\n",
    "    /* Column header styling for all tables */\n",
    "    thead tr th {\n",
    "        background-color: #0b6da4 !important;\n",
    "        color: white !important;\n",
    "        font-weight: bold !important;\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    /* Table cell alignment */\n",
    "    tbody td {\n",
    "        text-align: center;\n",
    "    }\n",
    "\n",
    "    /* Remove spacing from top */\n",
    "    .block-container {\n",
    "        padding-top: 2rem;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# THEME COLORS\n",
    "# -----------------------\n",
    "CARD_COLOR = \"#d6f5f2\"\n",
    "TEXT_COLOR = \"#0b6da4\"\n",
    "SUMMARY_COLOR = \"#444\"\n",
    "\n",
    "# -----------------------\n",
    "# LOAD DATA\n",
    "# -----------------------\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"matched_df_final_filt.csv\", parse_dates=['timestamp'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df['sentiment_score'] = pd.to_numeric(df['sentiment_score'], errors='coerce')\n",
    "    df['phrase_freq'] = pd.to_numeric(df['phrase_freq'], errors='coerce')\n",
    "    df['trend_score'] = df['sentiment_score'].fillna(0) + df['phrase_freq'].fillna(0)\n",
    "    df['sentiment_label'] = df['sentiment_score'].apply(lambda x: 'Positive' if x == 1 else 'Negative' if x == -1 else 'Neutral')\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['month'] = df['timestamp'].dt.to_period('M').astype(str)\n",
    "    df['week'] = df['timestamp'].dt.strftime('%Y-%U')\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# -----------------------\n",
    "# HEADER\n",
    "# -----------------------\n",
    "st.markdown(\n",
    "    f\"\"\"\n",
    "    <h1 style='text-align: center; color: {TEXT_COLOR};'>üìä TrendNav AI: E-commerce Opportunity Scanner</h1>\n",
    "    <p style='text-align: center; color: {SUMMARY_COLOR}; font-size:20px;'>\n",
    "        Identifying <strong>trending product demands</strong> using Reddit & Amazon QA,\n",
    "        and mapping them against <strong>inventory signals</strong> to find high-opportunity areas for sellers.\n",
    "    </p>\n",
    "    \"\"\", unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# KPI CARDS (HORIZONTAL)\n",
    "# -----------------------\n",
    "# KPIs should use unfiltered data\n",
    "top_phrase_unfiltered = (\n",
    "    df.groupby(['clean_phrase', 'matched_product'])\n",
    "    .agg(total_mentions=('phrase_freq', 'sum'), avg_sentiment=('sentiment_score', 'mean'), trend_score=('trend_score', 'sum'))\n",
    "    .reset_index()\n",
    "    .sort_values('trend_score', ascending=False)\n",
    ")\n",
    "top_row_unfiltered = top_phrase_unfiltered.iloc[0] if not top_phrase_unfiltered.empty else {}\n",
    "\n",
    "kpi1, kpi2, kpi3 = st.columns(3)\n",
    "\n",
    "with kpi1:\n",
    "    keyword = top_row_unfiltered['clean_phrase'] if not top_phrase_unfiltered.empty else 'N/A'\n",
    "    subcat = top_row_unfiltered['matched_product'] if not top_phrase_unfiltered.empty else ''\n",
    "    st.markdown(f\"\"\"\n",
    "    <div style=\"background-color:{CARD_COLOR}; padding:15px; height:140px; border-radius:12px;\">\n",
    "        <h5 style=\"color:{TEXT_COLOR};\">Top Keyword</h5>\n",
    "        <h3>{keyword} <span style='font-size: 16px; color: #777;'>({subcat})</span></h3>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "with kpi2:\n",
    "    total_mentions = int(top_row_unfiltered['total_mentions']) if not top_phrase_unfiltered.empty else 0\n",
    "    st.markdown(f\"\"\"\n",
    "    <div style=\"background-color:{CARD_COLOR}; padding:15px; height:140px; border-radius:12px;\">\n",
    "        <h5 style=\"color:{TEXT_COLOR};\">Total Mentions</h5>\n",
    "        <h3>{total_mentions}</h3>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "with kpi3:\n",
    "    avg_sentiment = round(top_row_unfiltered['avg_sentiment'], 2) if not top_phrase_unfiltered.empty else 0.0\n",
    "    st.markdown(f\"\"\"\n",
    "    <div style=\"background-color:{CARD_COLOR}; padding:15px; height:140px; border-radius:12px;\">\n",
    "        <h5 style=\"color:{TEXT_COLOR};\">Avg Sentiment</h5>\n",
    "        <h3>{avg_sentiment}</h3>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# FILTERS\n",
    "# -----------------------\n",
    "# one space row to make the space \n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <p style='text-align:center; font-size:20px; color:#333;'>\n",
    "    üí° Want to see what people <strong>love</strong> or <strong>complain</strong> about?  \n",
    "    Use the filters to explore trending products by sentiment.\n",
    "    </p>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "# 3 filters in a row\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "with col1:\n",
    "    sentiment_choice = st.radio(\"üß† Sentiment:\", ['All', 'Positive', 'Neutral', 'Negative'], horizontal=True, index=0)\n",
    "\n",
    "with col2:\n",
    "    time_choice = st.radio(\"‚è±Ô∏è Time View:\", ['Daily', 'Weekly', 'Monthly'], horizontal=True)\n",
    "\n",
    "with col3:\n",
    "    all_subcats = sorted(df['matched_product'].dropna().unique())\n",
    "    # Multiselect dropdown sorted by trend score\n",
    "    sorted_subcats = (\n",
    "        df.groupby('matched_product')['trend_score']\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .index.tolist()\n",
    "    )\n",
    "\n",
    "    selected_subcats = st.multiselect(\n",
    "        \"üßµ Subcategory:\",\n",
    "        options=sorted_subcats,\n",
    "        default=sorted_subcats[:5],  # Optional: pre-select top 5\n",
    "        help=\"Filter by one or more top-ranked subcategories\"\n",
    "    )\n",
    "\n",
    "    \n",
    "# -----------------------\n",
    "# APPLY FILTERS to create df_filtered\n",
    "# -----------------------\n",
    "df_filtered = df.copy()\n",
    "\n",
    "if sentiment_choice != 'All':\n",
    "    df_filtered = df_filtered[df_filtered['sentiment_label'] == sentiment_choice]\n",
    "\n",
    "if selected_subcats:\n",
    "    df_filtered = df_filtered[df_filtered['matched_product'].isin(selected_subcats)]\n",
    "\n",
    "\n",
    "# Apply time view\n",
    "if time_choice == 'Weekly':\n",
    "    df_filtered['time_unit'] = df_filtered['week']\n",
    "elif time_choice == 'Monthly':\n",
    "    df_filtered['time_unit'] = df_filtered['month']\n",
    "else:\n",
    "    df_filtered['time_unit'] = df_filtered['date']\n",
    "\n",
    "# -----------------------\n",
    "# TIME FILTER\n",
    "# -----------------------\n",
    "# time_choice = st.radio(\"‚è±Ô∏è Time View:\", ['Daily', 'Weekly', 'Monthly'], horizontal=True)\n",
    "if time_choice == 'Weekly':\n",
    "    df_filtered['time_unit'] = df_filtered['week']\n",
    "elif time_choice == 'Monthly':\n",
    "    df_filtered['time_unit'] = df_filtered['month']\n",
    "else:\n",
    "    df_filtered['time_unit'] = df_filtered['date']\n",
    "\n",
    "# -----------------------\n",
    "# SUMMARY LINE\n",
    "# -----------------------\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "\n",
    "summary_map = {\n",
    "    'Positive': \"üåø Sentiment is strongly positive ‚Äî these products are resonating well!\",\n",
    "    'Negative': \"‚ö†Ô∏è Negative feedback signals product improvement potential.\",\n",
    "    'Neutral': \"üü° Moderate opinions ‚Äî could go either way!\",\n",
    "    'All': \"üìä Viewing combined sentiment ‚Äî ideal for overall trend monitoring.\"\n",
    "}\n",
    "st.markdown(f\"<p style='text-align:center; color:#333; font-size:20px;'>{summary_map.get(sentiment_choice)}</p>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# TRENDING TABLE + CHART\n",
    "# -----------------------\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "\n",
    "left_col, right_col = st.columns([2, 2.2])\n",
    "\n",
    "with left_col:\n",
    "    st.markdown(\"### üî• Top Trending Products\")\n",
    "    top_keywords = (\n",
    "        df_filtered.groupby(['matched_product', 'clean_phrase'])\n",
    "        .agg(total_mentions=('phrase_freq', 'sum'),\n",
    "             avg_sentiment=('sentiment_score', 'mean'),\n",
    "             trend_score=('trend_score', 'sum'))\n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            'matched_product': 'Product Subcategory',\n",
    "            'clean_phrase': 'Product Keywords',\n",
    "            'total_mentions': 'Total Mentions',\n",
    "            'avg_sentiment': 'Avg Sentiment',\n",
    "            'trend_score': 'Trend Score'\n",
    "        })\n",
    "        .sort_values('Trend Score', ascending=False)\n",
    "        .head(15)\n",
    "    )\n",
    "    st.dataframe(top_keywords, use_container_width=True)\n",
    "\n",
    "with right_col:\n",
    "    st.markdown(\"### üìà Trend Score Over Time\")\n",
    "    df_trend = df_filtered[df_filtered['timestamp'].dt.year < 2025].copy()\n",
    "    trend_data = (\n",
    "        df_trend.groupby(['time_unit', 'matched_product'])\n",
    "        .agg(trend_score=('trend_score', 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    chart = alt.Chart(trend_data).mark_line().encode(\n",
    "    x=alt.X('time_unit:T', title='Date', axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('trend_score:Q', title='Trend Score', axis=alt.Axis(grid=False)),\n",
    "    color='matched_product:N',\n",
    "    tooltip=['time_unit:T', 'matched_product:N', 'trend_score:Q']\n",
    ").properties(\n",
    "    width=520,\n",
    "    height=390,\n",
    "    background='#FFFFFF'  # ‚úÖ PERFECT MATCH FROM YOUR TABLE\n",
    ").configure_axis(\n",
    "    labelColor='#444',\n",
    "    titleColor='#0b6da4'\n",
    ").configure_legend(\n",
    "    labelColor='#333',\n",
    "    titleColor='#0b6da4'\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")\n",
    "\n",
    "\n",
    "    st.altair_chart(chart, use_container_width=True)\n",
    "\n",
    "# -----------------------\n",
    "# OPPORTUNITY ANALYSIS\n",
    "# -----------------------\n",
    "st.markdown(\"### üì¶ Opportunity Analysis\")\n",
    "\n",
    "# Use unfiltered for stock-opportunity logic\n",
    "median_stock = df['stock_level'].median()\n",
    "median_trend = df['trend_score'].median()\n",
    "\n",
    "opportunity_df = (\n",
    "    df.groupby('matched_product')\n",
    "    .agg(avg_stock=('stock_level', 'mean'), trend_score=('trend_score', 'sum'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "high_opp = opportunity_df[(opportunity_df['avg_stock'] < median_stock) & (opportunity_df['trend_score'] > median_trend)]\n",
    "low_opp = opportunity_df[(opportunity_df['avg_stock'] > median_stock) & (opportunity_df['trend_score'] < median_trend)]\n",
    "\n",
    "col_high, col_low = st.columns(2)\n",
    "with col_high:\n",
    "    st.markdown(\"#### üü¢ High Opportunity Products\")\n",
    "    st.caption(\"These products are trending but have relatively low stock.\")\n",
    "    if not high_opp.empty:\n",
    "        st.dataframe(high_opp.rename(columns={'matched_product': 'Product Subcategory', 'avg_stock': 'Avg Stock', 'trend_score': 'Trend Score'}))\n",
    "    else:\n",
    "        st.info(\"No high opportunity products found.\")\n",
    "\n",
    "with col_low:\n",
    "    st.markdown(\"#### üìâ Declining Trends\")\n",
    "    st.caption(\"Products with a significant drop in trend score over the last **month**.\")\n",
    "\n",
    "    # Monthly trend\n",
    "    df['month'] = df['timestamp'].dt.to_period('M').astype(str)\n",
    "    monthly_trends = (\n",
    "        df.groupby(['matched_product', 'month'])['trend_score']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .sort_values(['matched_product', 'month'])\n",
    "    )\n",
    "    monthly_trends['pct_change'] = monthly_trends.groupby('matched_product')['trend_score'].pct_change()\n",
    "\n",
    "    recent_month_drop = (\n",
    "        monthly_trends.groupby('matched_product').tail(1)\n",
    "        .query('pct_change < -0.2')\n",
    "        .sort_values('pct_change')\n",
    "    )\n",
    "\n",
    "    if not recent_month_drop.empty:\n",
    "        display_df = recent_month_drop.rename(columns={\n",
    "            'matched_product': 'Product Subcategory',\n",
    "            'trend_score': 'Latest Trend Score',\n",
    "            'pct_change': '% Change'\n",
    "        })[['Product Subcategory', 'Latest Trend Score', '% Change']]\n",
    "\n",
    "        def color_decline(val):\n",
    "            color = 'red' if val < 0 else 'green'\n",
    "            return f'color: {color}'\n",
    "\n",
    "        def center_bold_header():\n",
    "            return [\n",
    "                {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n",
    "                {'selector': 'td', 'props': [('text-align', 'center')]}\n",
    "            ]\n",
    "\n",
    "        st.dataframe(\n",
    "            display_df\n",
    "            .style\n",
    "            .set_table_styles(center_bold_header())\n",
    "            .applymap(color_decline, subset=['% Change'])\n",
    "            .format({'Latest Trend Score': '{:,.0f}', '% Change': '{:.0%}'}),\n",
    "            use_container_width=True\n",
    "        )\n",
    "    else:\n",
    "        st.info(\"No declining trends detected this month.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
